# Before running, install required packages:
{% if notebook %}

!
{%- else %}
#
{%- endif %}
 pip install tianshou {% if env == "Box2D" %}box2d-py{% endif %}
{% if notebook %}


# ---
{% endif %}

import os
{% if env == "Classic control" or env == "Box2D" %}
import gym
{% endif %}
import torch
import pprint
import numpy as np
from torch.utils.tensorboard import SummaryWriter
{% if policy == "DQN" %}
from tianshou.policy import DQNPolicy
    {% if env == "Atari" %}
from tianshou.utils.net.discrete import DQN
    {% endif %}
{% endif %}
from tianshou.env import SubprocVectorEnv
{% if env == "Classic control" or env == "Box2D" %}
from tianshou.utils.net.common import Net
{% endif %}
from tianshou.trainer import offpolicy_trainer
from tianshou.data import Collector{% if prioritized == False %}, ReplayBuffer{% else %}, PrioritizedReplayBuffer{% endif %}


{% if env == "Atari" %}
from atari_wrapper import wrap_deepmind
# you can retrieve this file from: https://github.com/thu-ml/tianshou/blob/master/examples/atari/atari_wrapper.py


def make_atari_env(watch=False):
    if watch:
        return wrap_deepmind("{{ task }}", frame_stack={{frames_stack}},
                             episode_life=False, clip_rewards=False)
    return wrap_deepmind("{{ task }}", frame_stack={{frames_stack}})

{% endif %}

if __name__ == '__main__':
    # in this folder will be saved the best model and/or tensorboard files
    logdir = "log"
    {% if gpu == True %}
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    {% endif %}
    {% if env == "Atari" %}
    env = make_atari_env()
    {% endif %}
    {% if env == "Classic control" or env == "Box2D" %}
    task = "{{task}}"
    env = gym.make(task)
    {% endif %}
    state_shape = env.observation_space.shape or env.observation_space.n
    action_shape = env.env.action_space.shape or env.env.action_space.n
    print("Observations shape:", state_shape)
    print("Actions shape:", action_shape)
    # make environments
    {% if env == "Atari" %}
    train_envs = SubprocVectorEnv([lambda: make_atari_env()
                                   for _ in range({{training_num}})])
    test_envs = SubprocVectorEnv([lambda: make_atari_env(watch=True)
                                  for _ in range({{test_num}})])
    {% endif %}
    {% if env == "Classic control" or env == "Box2D" %}
    train_envs = SubprocVectorEnv([lambda: gym.make(task)
                                   for _ in range({{training_num}})])
    test_envs = SubprocVectorEnv([lambda: gym.make(task)
                                  for _ in range({{test_num}})])
    {% endif %}
    # seed
    np.random.seed({{seed}})
    torch.manual_seed({{seed}})
    train_envs.seed({{seed}})
    test_envs.seed({{seed}})
    # define model
    {% if policy == "DQN" %}
        {% if env == "Atari" %}
    net = DQN(*state_shape,
              action_shape, device).to(device)
        {% endif %}
        {% if env == "Classic control" or env == "Box2D" %}
    layers_num = {{layer_num}}
    net = Net(layers_num, state_shape,
              action_shape, device).to(device)
        {% endif %}
    {% endif %}
    optim = torch.optim.{{optimizer}}(net.parameters(), lr={{lr}})
    # define policy
    {% if policy == "DQN" %}
    policy = DQNPolicy(net, optim, discount_factor={{discount_factor}},
                       estimation_step={{estimation_step}},
                       target_update_freq={{target_update_freq}})
    {% endif %}
    # replay buffer: `save_last_obs` and `stack_num` can be removed together
    # when you have enough RAM
    {% if policy == "DQN" %}
        {% if prioritized == False %}
    buffer = ReplayBuffer({{buffer_size}}{% if env == "Classic control" or env == "Box2D" %}){% endif %}{% if env == "Atari" %},
                          ignore_obs_next=True,
                          save_only_last_obs=True,
                          stack_num={{frames_stack}})
                          {% endif %}
        {% else %}
    buffer = PrioritizedReplayBuffer({{buffer_size}},
                                     alpha={{alpha}}, beta={{beta}}{% if env == "Classic control" or env == "Box2D" %}){% endif %}{% if env == "Atari" %},
                                     ignore_obs_next=True,
                                     save_only_last_obs=True,
                                     stack_num={{frames_stack}})
                                     {% endif %}
        {% endif %}
    {% endif %}
    # collector
    train_collector = Collector(policy, train_envs, buffer)
    test_collector = Collector(policy, test_envs)
    # log
    log_path = os.path.join(logdir, '{{task}}', '{{policy}}')
    {% if tensorboard == True %}
    writer = SummaryWriter(log_path)
    {% endif %}

    {% if save == True %}
    def save_fn(policy):
        torch.save(policy, os.path.join(log_path, 'policy.pth'))

    {% endif %}
    {% if early_stop == True %}
    def stop_fn(mean_rewards):
        return mean_rewards >= {{target_reward}}

    {% endif %}
    def train_fn(epoch, env_step):
        eps_train = {{eps_train}}
        eps_train_final = {{eps_train_final}}
        linear_decay_steps = {{linear_decay_steps}}
        # nature DQN setting, linear decay in the first 1M steps
        if env_step <= linear_decay_steps:
            eps = eps_train - env_step / linear_decay_steps * \
                (eps_train - eps_train_final)
        else:
            eps = eps_train_final
        policy.set_eps(eps)
        {% if tensorboard == True %}
        writer.add_scalar('train/eps', eps, global_step=env_step)
        {% endif %}

    def test_fn(epoch, env_step):
        policy.set_eps({{eps_test}})

    {% if watch == True %}
    # watch agent's performance
    def watch():
        print("Testing agent ...")
        policy.eval()
        policy.set_eps({{eps_test}})
        test_envs.seed({{seed}})
        test_collector.reset()
        result = test_collector.collect(n_episode=[1] * {{test_num}},
                                        render=0)
        pprint.pprint(result)
    {% endif %}

    # test train_collector and start filling replay buffer
    {% if policy == "DQN" %}
    train_collector.collect(n_step={{batch_size}} * 4)
    {% endif %}
    # trainer
    result = offpolicy_trainer(
        policy, train_collector, test_collector,
        max_epoch={{epoch}},
        step_per_epoch={{step_per_epoch}},
        collect_per_step={{collect_per_step}},
        episode_per_test={{test_num}},
        batch_size={{batch_size}},
        train_fn=train_fn,
        test_fn=test_fn,
        {% if early_stop == True %}
        stop_fn={{stop_fn}}
        {% endif %}
        {% if save == True %}
        save_fn=save_fn,
        {% endif %}
        {% if tensorboard == True %}
        writer=writer,
        {% endif %}
        test_in_train=False)

    pprint.pprint(result)
    {% if watch == True %}
    watch()
    {% endif %}

